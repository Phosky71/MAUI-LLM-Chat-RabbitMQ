# MAUI-LLM-Chat-RabbitMQ

## ğŸ“ DescripciÃ³n

Sistema de comunicaciÃ³n entre mÃºltiples modelos de lenguaje (LLM) utilizando **.NET MAUI**, **RabbitMQ** y arquitectura **MVVM**. Permite que dos o mÃ¡s LLMs mantengan conversaciones automÃ¡ticas entre sÃ­ mediante un sistema de mensajerÃ­a en tiempo real.

**Tarea 4 - IntegraciÃ³n de LLM en Local**

---

## âœ¨ CaracterÃ­sticas

- âœ… AplicaciÃ³n mÃ³vil multiplataforma con .NET MAUI
- âœ… Arquitectura MVVM completa
- âœ… ComunicaciÃ³n en tiempo real con RabbitMQ
- âœ… IntegraciÃ³n con LLM local (LM Studio)
- âœ… ConfiguraciÃ³n dinÃ¡mica del modelo
- âœ… Conversaciones automÃ¡ticas entre LLMs
- âœ… Posibilidad de conectar con otros usuarios
- ğŸ”œ Mejora opcional: Texto a voz (TTS)

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

| TecnologÃ­a | VersiÃ³n | PropÃ³sito |
|-----------|---------|----------|
| .NET MAUI | 8.0+ | Framework multiplataforma |
| C# | 12.0 | Lenguaje principal |
| RabbitMQ | 3.x | Broker de mensajes |
| Docker | Latest | Contenedor RabbitMQ |
| LM Studio | Latest | LLM local |
| RabbitMQ.Client | 6.8+ | Cliente NuGet |
| CommunityToolkit.Mvvm | 8.2+ | MVVM Helpers |

---

## ğŸ“Š Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   App MAUI 1 (LLM1)   â”‚
â”‚                      â”‚
â”‚  [MainViewModel]      â”‚
â”‚  [RabbitMQ Service]   â”‚
â”‚  [LLM Service]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚  Publica/Suscribe
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚ RabbitMQ â”‚
    â”‚  Docker  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   App MAUI 2 (LLM2)   â”‚
â”‚                      â”‚
â”‚  [MainViewModel]      â”‚
â”‚  [RabbitMQ Service]   â”‚
â”‚  [LLM Service]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Estructura del Proyecto

Puedes ver la estructura completa en [ESTRUCTURA_PROYECTO.md](ESTRUCTURA_PROYECTO.md)

---

## ğŸš€ GuÃ­a de Inicio RÃ¡pido

### 1ï¸âƒ£ Prerequisitos

- **.NET 8 SDK** instalado
- **Docker Desktop** corriendo
- **LM Studio** instalado y configurado
- **Visual Studio 2022** o **VS Code** con extensiÃ³n C#

### 2ï¸âƒ£ Clonar el Repositorio

```bash
git clone https://github.com/Phosky71/MAUI-LLM-Chat-RabbitMQ.git
cd MAUI-LLM-Chat-RabbitMQ
```

### 3ï¸âƒ£ Iniciar RabbitMQ con Docker

```bash
cd docker
docker-compose up -d
```

Verifica que RabbitMQ estÃ© corriendo:
- **Management UI**: http://localhost:15672
- **Usuario**: admin
- **ContraseÃ±a**: admin123

### 4ï¸âƒ£ Configurar LM Studio

1. Abre LM Studio
2. Descarga un modelo (recomendado: Llama 3.2 o Phi-3)
3. Inicia el servidor local en el puerto **1234**
4. Verifica que estÃ© corriendo: http://localhost:1234/v1/models

### 5ï¸âƒ£ Compilar y Ejecutar

```bash
dotnet build
dotnet run
```

---

## ğŸ’» Comandos Ãštiles

### Docker

```bash
# Iniciar RabbitMQ
docker-compose up -d

# Detener RabbitMQ
docker-compose down

# Ver logs
docker-compose logs -f

# Reiniciar contenedor
docker-compose restart
```

### .NET MAUI

```bash
# Crear nuevo proyecto MAUI
dotnet new maui -n LLMChat.MAUI

# Agregar paquetes NuGet
dotnet add package RabbitMQ.Client
dotnet add package CommunityToolkit.Mvvm
dotnet add package Newtonsoft.Json

# Compilar
dotnet build

# Ejecutar en Android
dotnet build -t:Run -f net8.0-android

# Ejecutar en Windows
dotnet build -t:Run -f net8.0-windows10.0.19041.0
```

---

## ğŸ“š DocumentaciÃ³n Adicional

- [Estructura del Proyecto](ESTRUCTURA_PROYECTO.md)
- [CÃ³digo Completo](CODIGO_COMPLETO.md)
- [Ejemplos de Uso](docs/EJEMPLOS.md)

---

## ğŸ¯ Ejemplos de ConversaciÃ³n

### Debate: Gatos vs Perros

```
LLM1 (Pro-Gatos): "Los gatos son mascotas superiores por su independencia..."
LLM2 (Pro-Perros): "Los perros son compaÃ±eros mÃ¡s leales porque..."
```

### Juego: Tres en Raya

```
LLM1: "Coloco mi ficha en la posiciÃ³n central: [1,1]"
LLM2: "Respondo colocando mi ficha en: [0,0]"
```

---

## ğŸ”§ ConfiguraciÃ³n

En la **pantalla de configuraciÃ³n** puedes modificar:

- **Modelo LLM**: Seleccionar quÃ© modelo usar
- **Temperatura**: 0.0 - 1.0 (creatividad)
- **Max Tokens**: LÃ­mite de respuesta
- **System Prompt**: Personalidad del LLM
- **RabbitMQ**: Host, puerto, credenciales
- **Cola**: Nombre de la cola de mensajes

---

## âš™ï¸ Arquitectura MVVM

### Models
- **LLMConfig**: ConfiguraciÃ³n del modelo
- **ChatMessage**: Mensaje individual
- **RabbitMQConfig**: ParÃ¡metros de RabbitMQ

### ViewModels
- **MainViewModel**: LÃ³gica del chat
- **ConfigViewModel**: LÃ³gica de configuraciÃ³n

### Views
- **MainPage.xaml**: Interfaz principal
- **ConfigPage.xaml**: Pantalla de ajustes

### Services
- **RabbitMQService**: ComunicaciÃ³n con RabbitMQ
- **LLMService**: ComunicaciÃ³n con LM Studio

---

## ğŸ‘¥ Colaboradores

- **Phosky71** - Desarrollo inicial

---

## ğŸ“ Licencia

Este proyecto es educativo y se distribuye bajo licencia MIT.

---

## ğŸ“ Contacto

Para preguntas o sugerencias sobre este proyecto educativo, por favor abre un issue en el repositorio.

---

âš ï¸ **Nota**: Este es un proyecto educativo para la **Tarea 4 - IntegraciÃ³n de LLM en Local**.
